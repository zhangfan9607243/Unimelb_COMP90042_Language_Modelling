{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Language Modelling in Hangman Game\n",
    "\n",
    "This project implements an AI player for the classic Hangman word guessing game using character-level n-gram language models. The goal is to create automatic strategies that minimize incorrect guesses by learning patterns from text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Hangman Game Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Hangman game is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then. \n",
    "\n",
    "Here's a simple version of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "    This function plays the hangman game with the provided guesser and returns the number of incorrect guesses. \n",
    "    \n",
    "    secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "    guesser: a function which guesses the next character at each stage in the game. The function takes a:\n",
    "                - mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                - guessed: the set of characters which already been guessed in the game\n",
    "                - guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "    max_mistakes: limit on length of game, in terms of number of allowed mistakes\n",
    "    verbose: silent or verbose diagnostic prints\n",
    "    guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word and len(guess) == 1:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if len(guess) != 1:\n",
    "                    print('Please guess with only 1 character.')\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is a human guesser allowing interactive play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a simple function for manual play.\n",
    "    \"\"\"\n",
    "    print('\\nEnter your guess:')\n",
    "    return input().lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "interactive = False\n",
    "if interactive:\n",
    "    hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Data Preparation\n",
    "\n",
    "This section prepares the dataset for training and testing the language models. We use words from NLTK's Brown corpus, filtering for alphabetic words and splitting into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types in test = 1000\n",
      "Number of word types in train = 39234\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('brown')\n",
    "np.random.seed(1)\n",
    "\n",
    "# training_set stores the rest word types for training\n",
    "training_set = []\n",
    "# test_set stores 1000 word types for testing\n",
    "test_set = []\n",
    "\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "# Add all word types in Brown corpus into a words list\n",
    "words_list = []\n",
    "\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    if word.isalpha() and word not in words_list:\n",
    "        words_list.append(word)\n",
    "\n",
    "# Rando shuffle the words list and split it into a training set and a testing set\n",
    "np.random.shuffle(words_list)\n",
    "\n",
    "training_set = words_list[:-1000]\n",
    "test_set = words_list[-1000:]\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(\"Number of word types in test =\", len(test_set))\n",
    "print(\"Number of word types in train =\", len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 Baseline Random Guesser Model\n",
    "\n",
    "This section implements a baseline random guessing strategy for comparison with more advanced language model-based approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_guesser(guesser, test):\n",
    "    \"\"\"\n",
    "        This function takes a guesser and measures the average number of incorrect guesses made over all the words in the test_set. \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for word in test:\n",
    "        total += hangman(word, guesser, 26, False)\n",
    "    return total / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing word = status\n",
      "Number of mistakes made by the random guesser = 20\n",
      "\n",
      "Testing the random guesser using every word in test set\n",
      "Average number of incorrect guesses:  16.777\n"
     ]
    }
   ],
   "source": [
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "\n",
    "    # Create a set that contains all letters\n",
    "    chr_set = {\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"}\n",
    "\n",
    "    # Create a set of letters available to choose that is the difference set of all letters and guessed letters\n",
    "    chr_available_list = list(chr_set - guessed)\n",
    "\n",
    "    # Randomly choose a letter from list of available letters and return it\n",
    "    chr_guess = np.random.choice(chr_available_list)\n",
    "\n",
    "    return chr_guess\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "random_word = np.random.choice(test_set)\n",
    "print(\"Guessing word =\", random_word)\n",
    "print(\"Number of mistakes made by the random guesser =\", hangman(random_word, random_guesser, 26, False))\n",
    "\n",
    "result = test_guesser(random_guesser, test_set)\n",
    "print(\"\\nTesting the random guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 Unigram Language Model\n",
    "\n",
    "This section implements a basic unigram language model that predicts characters based on their overall frequency in the training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the unigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  10.808\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "# Create an initial character count dictionary for reuse\n",
    "INIT_DICT = {\"a\":0,\"b\":0,\"c\":0,\"d\":0,\"e\":0,\"f\":0,\"g\":0,\"h\":0,\"i\":0,\"j\":0,\"k\":0,\"l\":0,\"m\":0,\n",
    "             \"n\":0,\"o\":0,\"p\":0,\"q\":0,\"r\":0,\"s\":0,\"t\":0,\"u\":0,\"v\":0,\"w\":0,\"x\":0,\"y\":0,\"z\":0}\n",
    "\n",
    "# Create a character count dictionary for uni-grams\n",
    "unigram_counts = INIT_DICT.copy()\n",
    "\n",
    "# Count the uni-gram characters in training set\n",
    "for word in training_set:\n",
    "    for chr in word:\n",
    "        unigram_counts[chr] += 1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def unigram_guesser(mask, guessed, unigram_counts=unigram_counts):\n",
    "\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "\n",
    "    # Initialize the character with maximum uni-gram count\n",
    "    max_chr = \"\"\n",
    "    max_count = 0\n",
    "\n",
    "    # Find the character with maximum uni-gram count and return it\n",
    "    for chr in unigram_counts:\n",
    "        if chr not in guessed and unigram_counts[chr] >= max_count:\n",
    "            max_chr = chr\n",
    "            max_count = unigram_counts[chr]\n",
    "\n",
    "    return max_chr\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(unigram_guesser, test_set)\n",
    "print(\"Testing the unigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Unigram Length-Conditioned Model\n",
    "\n",
    "This section improves the unigram model by conditioning character frequencies on word length, recognizing that different word lengths have different character distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the length-conditioned unigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  10.706\n"
     ]
    }
   ],
   "source": [
    "unigram_counts_by_length = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "# Create a dictionary to store the uni-gram count for different word length\n",
    "unigram_counts_by_length = {}\n",
    "\n",
    "# For each word length, create a uni-gram count dictionary\n",
    "for word in training_set:\n",
    "\n",
    "    word_len = len(word)\n",
    "\n",
    "    if word_len in unigram_counts_by_length:\n",
    "        for chr in word:\n",
    "            unigram_counts_by_length[word_len][chr] += 1\n",
    "\n",
    "    else:\n",
    "        unigram_counts_by_length[word_len] = INIT_DICT.copy()\n",
    "        for chr in word:\n",
    "            unigram_counts_by_length[word_len][chr] += 1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def unigram_length_guesser(mask, guessed, unigram_counts_by_length=unigram_counts_by_length, unigram_counts=unigram_counts):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "\n",
    "    # Obtain the uni-gram count dictionary of the length for word to be guessed;\n",
    "    # If word length was never encountered in training set, use uni-gram count dictionary in question 3\n",
    "    mask_len = len(mask)\n",
    "\n",
    "    if mask_len in unigram_counts_by_length:\n",
    "        unigram_counts_dict = unigram_counts_by_length[mask_len]\n",
    "    else:\n",
    "        unigram_counts_dict = unigram_counts\n",
    "\n",
    "    # Find the character with maximum uni-gram count and return it\n",
    "    max_chr = \"\"\n",
    "    max_count = 0\n",
    "\n",
    "    for chr in unigram_counts_dict:\n",
    "        if chr not in guessed and unigram_counts_dict[chr] >= max_count:\n",
    "            max_chr = chr\n",
    "            max_count = unigram_counts_dict[chr]\n",
    "\n",
    "    return max_chr\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(unigram_length_guesser, test_set)\n",
    "print(\"Testing the length-conditioned unigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4 Bigram Language Model\n",
    "\n",
    "This section implements a bigram language model that considers character sequences, using the previous character as context to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the bigram guesser using every word in test set\n",
      "Average number of incorrect guesses:  9.118\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "# Create a revised initial character count dictionary for reuse (including start character)\n",
    "INIT_DICT_REVISED_1 = {\"$\":0,\n",
    "                       \"a\":0,\"b\":0,\"c\":0,\"d\":0,\"e\":0,\"f\":0,\"g\":0,\"h\":0,\"i\":0,\"j\":0,\"k\":0,\"l\":0,\"m\":0,\n",
    "                       \"n\":0,\"o\":0,\"p\":0,\"q\":0,\"r\":0,\"s\":0,\"t\":0,\"u\":0,\"v\":0,\"w\":0,\"x\":0,\"y\":0,\"z\":0}\n",
    "\n",
    "# Part 1: Setup uni-gram count & uni-gram frequency\n",
    "# Create a dictionary for uni-gram count\n",
    "unigram_counts_revised = INIT_DICT_REVISED_1.copy()\n",
    "\n",
    "for word in training_set:\n",
    "    word = \"$\" + word\n",
    "    for chr in word:\n",
    "        unigram_counts_revised[chr] += 1\n",
    "\n",
    "# Create a dictionary for uni-gram frequency\n",
    "unigram_freq = INIT_DICT_REVISED_1.copy()\n",
    "\n",
    "all_chr_count = sum(unigram_counts_revised.values())\n",
    "for chr in unigram_freq:\n",
    "    chr_count = unigram_counts_revised[chr]\n",
    "    unigram_freq[chr] = chr_count / all_chr_count\n",
    "\n",
    "# Part 2: Setup bi-gram count & bi-gram frequency\n",
    "# Create a dictionary for bi-gram count\n",
    "bigram_counts = {}\n",
    "\n",
    "for word in training_set:\n",
    "    word = \"$\" + word\n",
    "    for i in range(len(word)-1):\n",
    "        bi_1 = word[i]\n",
    "        bi_2 = word[i+1]\n",
    "        if bi_1 in bigram_counts:\n",
    "            bigram_counts[bi_1][bi_2] += 1\n",
    "        else:\n",
    "            bigram_counts[bi_1] = INIT_DICT.copy()\n",
    "            bigram_counts[bi_1][bi_2] += 1\n",
    "\n",
    "# Create a dictionary for uni-gram frequency\n",
    "bigram_freq = {}\n",
    "\n",
    "for bi_1 in bigram_counts:\n",
    "    bigram_freq[bi_1] = INIT_DICT.copy()\n",
    "    bi_1_count = unigram_counts_revised[bi_1]\n",
    "    for bi_2 in bigram_counts[bi_1]:\n",
    "        bi_2_count = bigram_counts[bi_1][bi_2]\n",
    "        bigram_freq[bi_1][bi_2] = bi_2_count / bi_1_count\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def bigram_guesser(mask, guessed, bigram_freq=bigram_freq, unigram_freq=unigram_freq): # add extra arguments if needed\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "\n",
    "    # Revise the mask to add a start character at the beginning\n",
    "    mask = [\"$\"] + mask\n",
    "\n",
    "    # Create a set for all possible bi-gram combinations and a boolean flag for whether consider uni-gram frequency\n",
    "    bigram_set = set()\n",
    "    is_contain_unigram = False\n",
    "\n",
    "    for i in range(len(mask)-1):\n",
    "        bigram = bi_1, bi_2 = (mask[i], mask[i+1])\n",
    "        if bi_1 != \"_\" and bi_2 == \"_\":\n",
    "            bigram_set.add(bigram)\n",
    "        if bi_1 == \"_\" and bi_2 == \"_\":\n",
    "            is_contain_unigram = True\n",
    "\n",
    "    # Sum up bi-gram frequencies and potentially uni-gram frequency across all letters (a-z)\n",
    "    freq_dict = INIT_DICT.copy()\n",
    "\n",
    "    for chr in freq_dict:\n",
    "        freq = 0\n",
    "        for bigram in bigram_set:\n",
    "            bi_1 = bigram[0]\n",
    "            freq += bigram_freq[bi_1][chr]\n",
    "        if is_contain_unigram:\n",
    "            freq += unigram_freq[chr]\n",
    "        freq_dict[chr] = freq\n",
    "\n",
    "    # Find the letter with maximum total frequency and return it\n",
    "    max_chr = \"\"\n",
    "    max_freq = 0\n",
    "\n",
    "    for chr in freq_dict:\n",
    "        if chr not in guessed and freq_dict[chr] >= max_freq:\n",
    "            max_chr = chr\n",
    "            max_freq = freq_dict[chr]\n",
    "\n",
    "    return max_chr\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "result = test_guesser(bigram_guesser, test_set)\n",
    "print(\"Testing the bigram guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.5 Self-Defined Advanced Language Model\n",
    "\n",
    "This section explores more sophisticated approaches, including trigram models and other advanced techniques to improve guessing accuracy. The task is to define and implement your own advanced language model for Hangman, which could include techniques such as smoothing, backoff, or even neural network-based approaches.\n",
    "\n",
    "The method I use is the combination of 3 tri-gram models plus a uni-gram model, where the tri-gram models consider all possible probability models of 3 characters (use 1 & 2 to predict 3; use 1 & 3 to predict 2; use 2 and 3 to predict 1). The probability estimations from 4 models are summed up as in Question 5. I use tri-gram models because combinations of 3 characters deliver more lexical information than combinations of 2 characters. Also, uni-gram model is retained, because for short words, individual characters deliver enough lexical information.\n",
    "\n",
    "I have also tried the bidirectional bi-gram models, but the performance is not improved and even worse than not adding them. This also suggests that 3-character combination is more useful than 2-character combination.\n",
    "\n",
    "In my method, the average number of incorrect guesses is 7.45 on the testing set, on average if I try it on different split of training & testing set. This is lower than all methods in previous questions, which suggests that the tri-gram model is more suitable for developing AI for the Hangman game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing my amazing AI guesser using every word in test set\n",
      "Average number of incorrect guesses:  7.421\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "## Part 0: Prepare initial character count dictionaries\n",
    "# Create a revised initial character count dictionary for reuse (including start character)\n",
    "INIT_DICT_REVISED_1 = {\"$\":0,\n",
    "                       \"a\":0,\"b\":0,\"c\":0,\"d\":0,\"e\":0,\"f\":0,\"g\":0,\"h\":0,\"i\":0,\"j\":0,\"k\":0,\"l\":0,\"m\":0,\n",
    "                       \"n\":0,\"o\":0,\"p\":0,\"q\":0,\"r\":0,\"s\":0,\"t\":0,\"u\":0,\"v\":0,\"w\":0,\"x\":0,\"y\":0,\"z\":0}\n",
    "\n",
    "# Create a revised initial character count dictionary for reuse (including end character)\n",
    "INIT_DICT_REVISED_2 = {\"&\":0,\n",
    "                       \"a\":0,\"b\":0,\"c\":0,\"d\":0,\"e\":0,\"f\":0,\"g\":0,\"h\":0,\"i\":0,\"j\":0,\"k\":0,\"l\":0,\"m\":0,\n",
    "                       \"n\":0,\"o\":0,\"p\":0,\"q\":0,\"r\":0,\"s\":0,\"t\":0,\"u\":0,\"v\":0,\"w\":0,\"x\":0,\"y\":0,\"z\":0}\n",
    "\n",
    "# Create a revised initial character count dictionary for reuse (including both start and end character)\n",
    "INIT_DICT_REVISED_3 = {\"$\":0,\"&\":0,\n",
    "                       \"a\":0,\"b\":0,\"c\":0,\"d\":0,\"e\":0,\"f\":0,\"g\":0,\"h\":0,\"i\":0,\"j\":0,\"k\":0,\"l\":0,\"m\":0,\n",
    "                       \"n\":0,\"o\":0,\"p\":0,\"q\":0,\"r\":0,\"s\":0,\"t\":0,\"u\":0,\"v\":0,\"w\":0,\"x\":0,\"y\":0,\"z\":0}\n",
    "\n",
    "## Part 1: Setup uni-gram count & frequency\n",
    "u_gram_counts = INIT_DICT_REVISED_3.copy()\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for chr in word:\n",
    "        u_gram_counts[chr] += 1\n",
    "\n",
    "u_gram_freq = INIT_DICT_REVISED_3.copy()\n",
    "for chr in u_gram_freq:\n",
    "    u_gram_freq[chr] = u_gram_counts[chr] / sum(u_gram_counts.values())\n",
    "\n",
    "# Part 2: Setup tri-gram count & tri-gram frequency in 3 ways\n",
    "# Bi-gram count normal: two consecutive letters\n",
    "# e.g., (a,b,c,d) >> ab, bc, cd\n",
    "b_gram_count_normal = {}\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for i in range(len(word)-1):\n",
    "        bigram = word[i] + word[i+1]\n",
    "        if bigram in b_gram_count_normal:\n",
    "            b_gram_count_normal[bigram] += 1\n",
    "        else:\n",
    "            b_gram_count_normal[bigram] = 1\n",
    "\n",
    "# Bi-gram count middle: two spaced letters\n",
    "# e.g., (a,b,c,d) >> ac, bd\n",
    "b_gram_count_middle = {}\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for i in range(len(word)-2):\n",
    "        bigram = word[i] + word[i+2]\n",
    "        if bigram in b_gram_count_middle:\n",
    "            b_gram_count_middle[bigram] += 1\n",
    "        else:\n",
    "            b_gram_count_middle[bigram] = 1\n",
    "\n",
    "# Tri-gram: Forward\n",
    "# e.g., (a,b,c,d) >> ab:c, bc:d\n",
    "t_gram_counts_forwrd = {}\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for i in range(len(word)-2):\n",
    "        cnd = word[i] + word[i+1]\n",
    "        obj = word[i+2]\n",
    "        if cnd in t_gram_counts_forwrd:\n",
    "            t_gram_counts_forwrd[cnd][obj] += 1\n",
    "        else:\n",
    "            t_gram_counts_forwrd[cnd] = INIT_DICT_REVISED_2.copy()\n",
    "            t_gram_counts_forwrd[cnd][obj] += 1\n",
    "\n",
    "t_gram_freq_forwrd = {}\n",
    "for cnd in t_gram_counts_forwrd:\n",
    "    t_gram_freq_forwrd[cnd] = INIT_DICT_REVISED_2.copy()\n",
    "    for obj in t_gram_counts_forwrd[cnd]:\n",
    "        t_gram_freq_forwrd[cnd][obj] = t_gram_counts_forwrd[cnd][obj] / b_gram_count_normal[cnd]\n",
    "\n",
    "# Tri-gram: Backward\n",
    "# e.g., (a,b,c,d) >> bc:a, cd:b\n",
    "t_gram_counts_bckwrd = {}\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for i in range(len(word)-2):\n",
    "        cnd = word[i+1] + word[i+2]\n",
    "        obj = word[i]\n",
    "        if cnd in t_gram_counts_bckwrd:\n",
    "            t_gram_counts_bckwrd[cnd][obj] += 1\n",
    "        else:\n",
    "            t_gram_counts_bckwrd[cnd] = INIT_DICT_REVISED_1.copy()\n",
    "            t_gram_counts_bckwrd[cnd][obj] += 1\n",
    "\n",
    "t_gram_freq_bckwrd = {}\n",
    "for cnd in t_gram_counts_bckwrd:\n",
    "    t_gram_freq_bckwrd[cnd] = INIT_DICT_REVISED_1.copy()\n",
    "    for obj in t_gram_counts_bckwrd[cnd]:\n",
    "        t_gram_freq_bckwrd[cnd][obj] = t_gram_counts_bckwrd[cnd][obj] / b_gram_count_normal[cnd]\n",
    "\n",
    "# Tri-gram: Middle\n",
    "# e.g., (a,b,c,d) >> ac:b, bd:c\n",
    "t_gram_counts_middle = {}\n",
    "for word in training_set:\n",
    "    word = \"$\" + word + \"&\"\n",
    "    for i in range(len(word)-2):\n",
    "        cnd = word[i] + word[i+2]\n",
    "        obj = word[i+1]\n",
    "        if cnd in t_gram_counts_middle:\n",
    "            t_gram_counts_middle[cnd][obj] += 1\n",
    "        else:\n",
    "            t_gram_counts_middle[cnd] = INIT_DICT_REVISED_3.copy()\n",
    "            t_gram_counts_middle[cnd][obj] += 1\n",
    "\n",
    "t_gram_freq_middle = {}\n",
    "for cnd in t_gram_counts_middle:\n",
    "    t_gram_freq_middle[cnd] = INIT_DICT_REVISED_3.copy()\n",
    "    for obj in t_gram_counts_middle[cnd]:\n",
    "        t_gram_freq_middle[cnd][obj] = t_gram_counts_middle[cnd][obj] / b_gram_count_middle[cnd]\n",
    "\n",
    "## Part 3: Setup guesser\n",
    "def my_amazing_ai_guesser(mask, guessed):\n",
    "\n",
    "    # Revise the mask to add a start character at the beginning and an end character at the end\n",
    "    mask = [\"$\"] + mask + [\"&\"]\n",
    "\n",
    "    # Create a boolean flag for whether consider uni-gram frequency\n",
    "    is_contain_unigram = False\n",
    "    for i in range(len(mask)-1):\n",
    "        bi_1, bi_2 = (mask[i], mask[i+1])\n",
    "        if bi_1 == \"_\" and bi_2 == \"_\":\n",
    "            is_contain_unigram = True\n",
    "\n",
    "    # Create sets for all possible tri-gram combinations in three ways\n",
    "    trigram_forwrd_set = set()\n",
    "    trigram_bckwrd_set = set()\n",
    "    trigram_middle_set = set()\n",
    "    for i in range(len(mask)-2):\n",
    "        trigram = ti_1, ti_2, ti_3 = (mask[i], mask[i+1], mask[i+2])\n",
    "        if ti_1 != \"_\" and ti_2 != \"_\" and ti_3 == \"_\":\n",
    "            trigram_forwrd_set.add(trigram)\n",
    "        if ti_1 == \"_\" and ti_2 != \"_\" and ti_3 != \"_\":\n",
    "            trigram_bckwrd_set.add(trigram)\n",
    "        if ti_1 != \"_\" and ti_2 == \"_\" and ti_3 != \"_\":\n",
    "            trigram_middle_set.add(trigram)\n",
    "\n",
    "    # Sum up tri-gram frequencies in three ways and potentially uni-gram frequency across all letters (a-z)\n",
    "    freq_dict = INIT_DICT.copy()\n",
    "    for chr in freq_dict:\n",
    "        freq = 0\n",
    "        if is_contain_unigram:\n",
    "            freq += u_gram_freq[chr]\n",
    "        for trigram in trigram_forwrd_set:\n",
    "            base = trigram[0] + trigram[1]\n",
    "            freq += t_gram_freq_forwrd[base][chr]\n",
    "        for trigram in trigram_bckwrd_set:\n",
    "            base = trigram[1] + trigram[2]\n",
    "            freq += t_gram_freq_bckwrd[base][chr]\n",
    "        for trigram in trigram_middle_set:\n",
    "            base = trigram[0] + trigram[2]\n",
    "            freq += t_gram_freq_middle[base][chr]\n",
    "        freq_dict[chr] = freq\n",
    "\n",
    "    # Find the letter with maximum total frequency and return it\n",
    "    max_chr = \"\"\n",
    "    max_freq = 0\n",
    "    for chr in freq_dict:\n",
    "        if chr not in guessed and freq_dict[chr] >= max_freq:\n",
    "            max_chr = chr\n",
    "            max_freq = freq_dict[chr]\n",
    "\n",
    "    return max_chr\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "result = test_guesser(my_amazing_ai_guesser, test_set)\n",
    "print(\"Testing my amazing AI guesser using every word in test set\")\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
